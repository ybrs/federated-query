# Federated Query Engine

A production-grade federated query engine written in Python that executes SQL queries across multiple heterogeneous data sources (PostgreSQL, DuckDB) in an optimal way.

## Features

- **Multi-Source Querying**: Execute SQL queries across PostgreSQL and DuckDB data sources
- **Intelligent Optimization**: Cost-based query optimization with predicate pushdown, projection pushdown, and join reordering
- **Decorrelation**: Automatic transformation of correlated subqueries into efficient joins
- **Flexible Execution**: Support for broadcast joins, hash joins, and nested loop joins
- **Arrow-Based**: Uses Apache Arrow for efficient in-memory data representation and transfer
- **Production-Ready**: Comprehensive error handling, logging, and configuration

## Architecture

The engine follows a classic database architecture with several optimization phases:

```
SQL Query
   â†“
[Parser] (sqlglot) â†’ AST
   â†“
[Binder] â†’ Resolved Logical Plan
   â†“
[Pre-Optimizer] â†’ Simplified Plan
   â†“
[Decorrelator] â†’ Decorrelated Plan
   â†“
[Logical Optimizer] â†’ Optimized Logical Plan
   â†“
[Physical Planner] â†’ Physical Plan (with cost estimation)
   â†“
[Executor] â†’ Results (Arrow format)
```

## Project Structure

```
federated-query/
â”œâ”€â”€ federated_query/          # Main package
â”‚   â”œâ”€â”€ catalog/              # Metadata catalog
â”‚   â”œâ”€â”€ config/               # Configuration management
â”‚   â”œâ”€â”€ datasources/          # Data source connectors
â”‚   â”‚   â”œâ”€â”€ postgresql.py     # PostgreSQL connector
â”‚   â”‚   â””â”€â”€ duckdb.py         # DuckDB connector
â”‚   â”œâ”€â”€ executor/             # Query executor
â”‚   â”œâ”€â”€ optimizer/            # Query optimization
â”‚   â”‚   â”œâ”€â”€ rules.py          # Optimization rules
â”‚   â”‚   â”œâ”€â”€ cost.py           # Cost model
â”‚   â”‚   â”œâ”€â”€ decorrelation.py  # Subquery decorrelation
â”‚   â”‚   â””â”€â”€ statistics.py     # Statistics collection
â”‚   â”œâ”€â”€ parser/               # SQL parser and binder
â”‚   â”œâ”€â”€ plan/                 # Plan representations
â”‚   â”‚   â”œâ”€â”€ logical.py        # Logical plan nodes
â”‚   â”‚   â”œâ”€â”€ physical.py       # Physical plan nodes
â”‚   â”‚   â””â”€â”€ expressions.py    # Expression nodes
â”‚   â””â”€â”€ utils/                # Utilities
â”œâ”€â”€ config/                   # Configuration files
â”‚   â””â”€â”€ example_config.yaml   # Example configuration
â”œâ”€â”€ tests/                    # Test suite
â”œâ”€â”€ plan.md                   # Architecture documentation
â””â”€â”€ tasks.md                  # Implementation roadmap
```

## Installation

```bash
# Clone the repository
git clone <repository-url>
cd federated-query

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

## Configuration

Create a YAML configuration file defining your data sources:

```yaml
datasources:
  postgres_prod:
    type: postgresql
    host: localhost
    port: 5432
    database: analytics
    user: postgres
    password: postgres
    schemas: [public]
    capabilities: [aggregations, joins, window_functions]

  local_duckdb:
    type: duckdb
    path: /data/analytics.duckdb
    read_only: true
    capabilities: [aggregations, joins]

optimizer:
  enable_predicate_pushdown: true
  enable_join_reordering: true

executor:
  max_memory_mb: 2048
  batch_size: 10000
```

See `config/example_config.yaml` for a complete example.

## Usage

```python
from federated_query.config import load_config
from federated_query.catalog import Catalog
from federated_query.parser import Parser, Binder
from federated_query.optimizer import RuleBasedOptimizer
from federated_query.executor import Executor

# Load configuration
config = load_config("config.yaml")

# Initialize catalog
catalog = Catalog()
# ... register data sources and load metadata ...

# Parse and execute query
parser = Parser()
binder = Binder(catalog)
optimizer = RuleBasedOptimizer(catalog)
executor = Executor(config.executor)

sql = """
    SELECT o.id, c.name, o.amount
    FROM postgres_prod.public.orders o
    JOIN local_duckdb.main.customers c ON o.customer_id = c.id
    WHERE o.amount > 1000
"""

# Parse â†’ Bind â†’ Optimize â†’ Execute
ast = parser.parse(sql)
logical_plan = parser.ast_to_logical_plan(ast)
bound_plan = binder.bind(logical_plan)
optimized_plan = optimizer.optimize(bound_plan)
# ... generate physical plan ...
results = executor.execute_to_table(physical_plan)

print(results)
```

## Development Status

This project is under active development. See `tasks.md` for the implementation roadmap.

### Completed Phases

**Phase 0: Foundation** âœ…
- âœ… Project structure and skeleton
- âœ… Core abstractions (plans, expressions, data sources)
- âœ… Configuration system
- âœ… Basic catalog structure
- âœ… Data source connectors (PostgreSQL, DuckDB)
- âœ… Test infrastructure

**Phase 1: Basic Query Execution** âœ…
- âœ… Parser: AST to logical plan conversion
- âœ… Binder: Reference resolution with catalog integration
- âœ… Physical operators: Scan, Filter, Project, Limit
- âœ… Basic executor: Single-table queries
- âœ… End-to-end pipeline for simple SELECT queries
- âœ… All 65 tests passing

**Query Example (Phase 1):**
```sql
SELECT col1, col2 FROM datasource.schema.table WHERE col1 > 10 LIMIT 100
```

### Current Phase (Phase 2)
- ðŸš§ Joins across data sources
- ðŸš§ Hash join and nested loop join operators
- ðŸš§ Data gathering from multiple sources
- ðŸš§ Join strategy selection

### Planned
- Aggregations and grouping
- Query optimization (pushdown, reordering)
- Cost-based planning
- Decorrelation
- Advanced features (window functions, CTEs)

## Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=federated_query

# Run specific test file
pytest tests/test_parser.py
```

## Documentation

- **[plan.md](plan.md)**: Detailed architecture and design decisions
- **[tasks.md](tasks.md)**: Implementation roadmap broken down by phases

## Key Design Principles

1. **Immutable Plans**: All plan nodes are immutable for easier reasoning and thread safety
2. **Push-Down First**: Always try to push computation to data sources to minimize data transfer
3. **Arrow-Based**: Use Apache Arrow for efficient columnar data representation
4. **Cost-Based**: Use statistics and cost models to choose optimal execution strategies
5. **Extensible**: Easy to add new data sources, optimization rules, and operators

## Performance Considerations

- **Predicate Pushdown**: Filters are pushed to data sources whenever possible
- **Projection Pushdown**: Only required columns are fetched from sources
- **Join Strategy Selection**: Automatically chooses between broadcast, hash, and nested loop joins
- **Parallel Execution**: Fetches from multiple data sources in parallel
- **Memory Management**: Spills to disk when memory limits are exceeded

## Contributing

Contributions are welcome! Please see the implementation roadmap in `tasks.md` for areas that need work.

## License

MIT License

## References

### Papers
- "Volcano - An Extensible and Parallel Query Evaluation System" (Graefe, 1994)
- "The Cascades Framework for Query Optimization" (Graefe, 1995)
- "Unnesting Arbitrary Queries" (Neumann & Kemper, 2015)

### Systems
- PostgreSQL query planner
- Apache Calcite (federated query framework)
- DuckDB optimizer
- Presto/Trino distributed query engine
